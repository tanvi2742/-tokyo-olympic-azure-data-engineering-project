# -tokyo-olympic-azure-data-engineering-project
**Project Overview**
This project focuses on designing and implementing a data pipeline to collect, transform, and store Olympic datasets for streamlined analytics and reporting. The workflow automates data ingestion, transformation, and storage, ensuring high-quality, analysis-ready datasets for downstream use.

**Key Features**
Ingested 5 different datasets into a centralized data lake for Olympic data.
Designed and implemented a data flow for efficient data collection and storage.
Optimized data transformations to generate clean, enriched datasets.
Enabled faster access for analysis and reporting, empowering stakeholders with data-driven insights.

**Technology Stack**
Azure Data Factory (ADF) – Orchestrating data pipelines
Azure Databricks (PySpark, Python) – Data transformation & processing
Azure Data Lake Storage Gen2 – Centralized data storage
Azure Synapse Analytics (SQL) – Querying and analytics
Python & SQL – Scripting and data manipulation

**Architecture Flow**
Data Ingestion – Using ADF to collect 5 raw datasets.
Data Storage – Raw data stored in Data Lake Gen2.
Data Transformation – Databricks notebooks (PySpark + Python) clean and enrich data.
Data Modeling & Querying – Processed data stored in Synapse Analytics for easy access.
Insights & Reporting – Analysis-ready datasets for decision-making.
